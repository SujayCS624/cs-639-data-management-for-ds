{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nprint(\"GPU available:\", torch.cuda.is_available())\nprint(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:58:02.538964Z","iopub.execute_input":"2025-05-07T19:58:02.539316Z","iopub.status.idle":"2025-05-07T19:58:02.543808Z","shell.execute_reply.started":"2025-05-07T19:58:02.539296Z","shell.execute_reply":"2025-05-07T19:58:02.542889Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:58:05.711328Z","iopub.execute_input":"2025-05-07T19:58:05.711602Z","iopub.status.idle":"2025-05-07T19:58:05.926281Z","shell.execute_reply.started":"2025-05-07T19:58:05.711581Z","shell.execute_reply":"2025-05-07T19:58:05.925538Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1493eccddd43425a9fcbd1387cc73daa"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip install bitsandbytes>=0.39.0\n!pip install --upgrade accelerate transformers datasets peft trl\n!pip install streamlit\n!npm install -g localtunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:58:19.992115Z","iopub.execute_input":"2025-05-07T19:58:19.992639Z","iopub.status.idle":"2025-05-07T19:58:45.375515Z","shell.execute_reply.started":"2025-05-07T19:58:19.992612Z","shell.execute_reply":"2025-05-07T19:58:45.374541Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting accelerate\n  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting datasets\n  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting peft\n  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Using cached trl-0.17.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.16)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.19.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, transformers, datasets, accelerate, trl, peft\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.5.0\n    Uninstalling datasets-3.5.0:\n      Successfully uninstalled datasets-3.5.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.3.0\n    Uninstalling accelerate-1.3.0:\n      Successfully uninstalled accelerate-1.3.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.6.0 datasets-3.6.0 fsspec-2025.3.0 peft-0.15.2 transformers-4.51.3 trl-0.17.0\nCollecting streamlit\n  Using cached streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.26.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nUsing cached streamlit-1.45.0-py3-none-any.whl (9.9 MB)\nUsing cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\nInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.45.0\n\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\nadded 22 packages in 2s\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K3 packages are looking for funding\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K  run `npm fund` for details\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.3.0\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.3.0\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.3.0\u001b[24m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!wget https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\n!unzip -o transcripts.zip -d transcripts/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:59:13.274324Z","iopub.execute_input":"2025-05-07T19:59:13.275144Z","iopub.status.idle":"2025-05-07T19:59:13.937094Z","shell.execute_reply.started":"2025-05-07T19:59:13.275114Z","shell.execute_reply":"2025-05-07T19:59:13.935922Z"}},"outputs":[{"name":"stdout","text":"--2025-05-07 19:59:13--  https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\nResolving github.com (github.com)... 140.82.116.3\nConnecting to github.com (github.com)|140.82.116.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip [following]\n--2025-05-07 19:59:13--  https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 290933 (284K) [application/zip]\nSaving to: ‘transcripts.zip’\n\ntranscripts.zip     100%[===================>] 284.11K  --.-KB/s    in 0.02s   \n\n2025-05-07 19:59:13 (12.8 MB/s) - ‘transcripts.zip’ saved [290933/290933]\n\nArchive:  transcripts.zip\n   creating: transcripts/transcripts/\n  inflating: transcripts/__MACOSX/._transcripts  \n  inflating: transcripts/transcripts/23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/__MACOSX/transcripts/._23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/transcripts/14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/transcripts/.DS_Store  \n  inflating: transcripts/__MACOSX/transcripts/._.DS_Store  \n  inflating: transcripts/transcripts/11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/__MACOSX/transcripts/._11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/transcripts/16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/transcripts/7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/transcripts/12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/__MACOSX/transcripts/._12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/transcripts/2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/__MACOSX/transcripts/._2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/transcripts/4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/transcripts/21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/transcripts/6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/transcripts/1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/transcripts/17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/__MACOSX/transcripts/._17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/transcripts/20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/transcripts/22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/__MACOSX/transcripts/._22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/transcripts/6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/transcripts/13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/__MACOSX/transcripts/._13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/transcripts/15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/__MACOSX/transcripts/._15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/transcripts/10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/__MACOSX/transcripts/._10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/transcripts/8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/__MACOSX/transcripts/._8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/transcripts/3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/__MACOSX/transcripts/._3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/transcripts/5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/__MACOSX/transcripts/._5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/transcripts/18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/transcripts/9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n  inflating: transcripts/__MACOSX/transcripts/._9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Q1.1: Load a 4-bit quantized Llama-3.2-1B-Instruct model and and its tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:59:25.777785Z","iopub.execute_input":"2025-05-07T19:59:25.778095Z","iopub.status.idle":"2025-05-07T19:59:30.881144Z","shell.execute_reply.started":"2025-05-07T19:59:25.778060Z","shell.execute_reply":"2025-05-07T19:59:30.880546Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:59:59.522270Z","iopub.execute_input":"2025-05-07T19:59:59.522695Z","iopub.status.idle":"2025-05-07T19:59:59.528247Z","shell.execute_reply.started":"2025-05-07T19:59:59.522670Z","shell.execute_reply":"2025-05-07T19:59:59.527405Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:00:03.586634Z","iopub.execute_input":"2025-05-07T20:00:03.587296Z","iopub.status.idle":"2025-05-07T20:00:03.592279Z","shell.execute_reply.started":"2025-05-07T20:00:03.587262Z","shell.execute_reply":"2025-05-07T20:00:03.591421Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:00:06.897476Z","iopub.execute_input":"2025-05-07T20:00:06.897777Z","iopub.status.idle":"2025-05-07T20:00:08.887143Z","shell.execute_reply.started":"2025-05-07T20:00:06.897754Z","shell.execute_reply":"2025-05-07T20:00:08.886304Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"617a83003225455ba8a52231d00d2345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1eaa85409c944649f93477e5b068a51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77394bd8b56e4ef8b98f910efd6ff28f"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,\n    low_cpu_mem_usage=True\n)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:00:12.890970Z","iopub.execute_input":"2025-05-07T20:00:12.891843Z","iopub.status.idle":"2025-05-07T20:00:37.803547Z","shell.execute_reply.started":"2025-05-07T20:00:12.891814Z","shell.execute_reply":"2025-05-07T20:00:37.802982Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57dd390ad44e40988907cf9f689da462"}},"metadata":{}},{"name":"stderr","text":"2025-05-07 20:00:15.271304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746648015.449929      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746648015.499717      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"841afb076a6841cc888a568047e7762b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e370867fef45e98e0d6963e8c8a830"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"Q1.2: Test your quantized model with different prompts (text generation).","metadata":{}},{"cell_type":"code","source":"def generate_response(prompt, max_new_tokens=100):\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:00:56.318584Z","iopub.execute_input":"2025-05-07T20:00:56.319452Z","iopub.status.idle":"2025-05-07T20:00:56.323385Z","shell.execute_reply.started":"2025-05-07T20:00:56.319424Z","shell.execute_reply":"2025-05-07T20:00:56.322536Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"prompt = \"What are some fun traditions or events that students at UW–Madison look forward to every year?\\n\\n\"\nresponse = generate_response(prompt, max_new_tokens=150)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:00:59.051250Z","iopub.execute_input":"2025-05-07T20:00:59.051527Z","iopub.status.idle":"2025-05-07T20:01:03.324357Z","shell.execute_reply.started":"2025-05-07T20:00:59.051506Z","shell.execute_reply":"2025-05-07T20:01:03.323722Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"What are some fun traditions or events that students at UW–Madison look forward to every year?\n\nHere are some fun traditions or events that students at UW–Madison look forward to every year:\n\n1. **Greek Week**: A week-long celebration of Greek life, with events like Greek dance performances, food festivals, and charity events.\n2. **Homecoming**: A high school event that takes place before the start of the fall semester, featuring football games, concerts, and other activities.\n3. **Columbus Day Parade**: A annual parade that takes place in the fall, featuring floats, marching bands, and other performances.\n4. **UW-Madison Football**: A annual football game that takes place in the fall, with the university's football team competing against other schools.\n5. **Wine and Cheese Tastings\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"prompt = \"What are some of the places to visit in Madison, Wisconsin?\\n\\n\"\nresponse = generate_response(prompt, max_new_tokens=150)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:01:07.866334Z","iopub.execute_input":"2025-05-07T20:01:07.866631Z","iopub.status.idle":"2025-05-07T20:01:11.498051Z","shell.execute_reply.started":"2025-05-07T20:01:07.866607Z","shell.execute_reply":"2025-05-07T20:01:11.497372Z"}},"outputs":[{"name":"stdout","text":"What are some of the places to visit in Madison, Wisconsin?\n\nMadison is a beautiful city with a rich history and cultural attractions. Here are some of the places to visit in Madison, Wisconsin:\n\n1. **University of Wisconsin-Madison**: The university is one of the largest in the country and offers a wide range of academic programs. Take a stroll through the beautiful campus and visit the iconic Mendel's Garden.\n2. **State Street**: This historic street is lined with shops, restaurants, and bars. It's a great place to explore and grab a bite to eat.\n3. **Boulevard Park**: A beautiful park with walking trails, a playground, and a lake. It's a great place to relax and enjoy the scenery.\n4. **The Olbrich Botanical Gardens**:\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"prompt = \"How can one improve their time management skills?\\n\\n\"\nresponse = generate_response(prompt, max_new_tokens=150)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:01:16.683595Z","iopub.execute_input":"2025-05-07T20:01:16.683907Z","iopub.status.idle":"2025-05-07T20:01:20.310758Z","shell.execute_reply.started":"2025-05-07T20:01:16.683885Z","shell.execute_reply":"2025-05-07T20:01:20.310115Z"}},"outputs":[{"name":"stdout","text":"How can one improve their time management skills?\n\nHere are some tips to improve your time management skills:\n\n1.  **Set clear goals**: Start by identifying what you want to achieve in a specific timeframe. This will help you focus on what's truly important and make sure you're spending your time wisely.\n2.  **Prioritize tasks**: Make a list of all your tasks and then prioritize them based on their importance and urgency. This will help you tackle the most critical tasks first.\n3.  **Use a planner or calendar**: Write down all your tasks, appointments, and deadlines in a planner or calendar. This will help you stay organized and keep track of your time.\n4.  **Break tasks into smaller chunks**: Large tasks can be overwhelming. Break them down into smaller,\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Q1.3: Identify a prompt where the model fails and analyze the failure.","metadata":{}},{"cell_type":"code","source":"prompt = \"If yesterday was two days before Monday, what day is today?\\n\\n\"\nresponse = generate_response(prompt, max_new_tokens=150)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:01:24.843386Z","iopub.execute_input":"2025-05-07T20:01:24.843672Z","iopub.status.idle":"2025-05-07T20:01:27.850787Z","shell.execute_reply.started":"2025-05-07T20:01:24.843653Z","shell.execute_reply":"2025-05-07T20:01:27.849991Z"}},"outputs":[{"name":"stdout","text":"If yesterday was two days before Monday, what day is today?\n\n## Step 1: Determine the number of days between yesterday and today.\nYesterday was two days before Monday, which means today is two days after Monday.\n\n## Step 2: Calculate the total number of days between yesterday and today.\nSince yesterday was two days before Monday and today is two days after Monday, the total number of days between yesterday and today is 2 + 2 = 4 days.\n\n## Step 3: Determine the day of the week that is four days after Monday.\nStarting from Monday, adding four days lands on Friday.\n\nThe final answer is: $\\boxed{Friday}$\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"The model failed for the above prompt possibly due to a couple of reasons:\n\n1) The model we are using is smaller and probably struggles with multi-step reasoning problems like the one above where you need to first deduce what day of the week yesterday was and then you need to figure out what today is.\n2) The model also lacks access to external sources like a calendar or other tools to test out its reasoning logic.","metadata":{}},{"cell_type":"markdown","source":"Q1.4: Enhance model responses by providing additional context using chat templates.","metadata":{}},{"cell_type":"code","source":"def apply_chat_template(role, prompt, max_new_tokens=100):\n    messages = [{\"role\": \"system\",\n                \"content\": role},\n                {\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    print(\"Role:\", role)\n    print(\"Prompt:\", prompt)\n    print(\"Generated Response:\", tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:02:07.177266Z","iopub.execute_input":"2025-05-07T20:02:07.177852Z","iopub.status.idle":"2025-05-07T20:02:07.182601Z","shell.execute_reply.started":"2025-05-07T20:02:07.177828Z","shell.execute_reply":"2025-05-07T20:02:07.181713Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"prompt = \"What should I choose as my starter if my rival who I want to beat chose Bulbasaur as their starter?\"\nrole = \"You are a wise and encouraging Pokémon professor who gives thoughtful advice to new trainers. Recommend them one Pokemon\"\napply_chat_template(role, prompt, max_new_tokens=300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:02:10.730257Z","iopub.execute_input":"2025-05-07T20:02:10.730819Z","iopub.status.idle":"2025-05-07T20:02:15.811482Z","shell.execute_reply.started":"2025-05-07T20:02:10.730791Z","shell.execute_reply":"2025-05-07T20:02:15.810779Z"}},"outputs":[{"name":"stdout","text":"Role: You are a wise and encouraging Pokémon professor who gives thoughtful advice to new trainers. Recommend them one Pokemon\nPrompt: What should I choose as my starter if my rival who I want to beat chose Bulbasaur as their starter?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are a wise and encouraging Pokémon professor who gives thoughtful advice to new trainers. Recommend them one Pokemonuser\n\nWhat should I choose as my starter if my rival who I want to beat chose Bulbasaur as their starter?assistant\n\nWhat an interesting situation. Considering Bulbasaur is a Grass-type Pokémon, I'd recommend a starter that complements its strengths.\n\nAs a rival, I'm sure they have a strong understanding of the Pokémon world. In that case, I'd suggest a starter that offers a different set of strengths and weaknesses.\n\nI recommend choosing Charmander as your starter. Charmander is a Fire-type Pokémon, which would provide a nice contrast to Bulbasaur's Grass-type abilities. Additionally, Charmander's Fire-type moves can help you take down Bulbasaur's grassy allies.\n\nCharmander's Fire-type moves can also help you take advantage of any weaknesses Bulbasaur may have. With Charmander by your side, you'll be able to take on Bulbasaur and potentially emerge victorious.\n\nRemember, the journey to victory is just as important as the destination. Choose a starter that you're passionate about, and don't be afraid to adapt to changing circumstances. Good luck, and may the best trainer win!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Yes, the prompt got the model to respond in a manner befitting of a Pokemon professor helping out the trainer at the start of their journey. It stayed in character and gave relevant strategic advice.","metadata":{}},{"cell_type":"markdown","source":"Q2.1: Test the model before fine-tuning.","metadata":{}},{"cell_type":"code","source":"prompt = \"Can you tell me what the six projects covered in CS 639 Data Management for Data Science are and what tools each one focuses on in less than a 100 words?\"\nrole = \"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\napply_chat_template(role, prompt, max_new_tokens=300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:00.578161Z","iopub.execute_input":"2025-05-07T20:03:00.578442Z","iopub.status.idle":"2025-05-07T20:03:07.171982Z","shell.execute_reply.started":"2025-05-07T20:03:00.578425Z","shell.execute_reply":"2025-05-07T20:03:07.171187Z"}},"outputs":[{"name":"stdout","text":"Role: You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\nPrompt: Can you tell me what the six projects covered in CS 639 Data Management for Data Science are and what tools each one focuses on in less than a 100 words?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nCan you tell me what the six projects covered in CS 639 Data Management for Data Science are and what tools each one focuses on in less than a 100 words?assistant\n\nIn CS 639 Data Management for Data Science, six projects are covered, including:\n\n1. **Data Cleaning**: Focuses on handling missing values, data normalization, and data preprocessing. Tools used include Pandas for data manipulation, NumPy for numerical computations, and Matplotlib for visualization.\n2. **Data Integration**: Covers data integration techniques, such as data merging, data transformation, and data denormalization. Tools include Pandas, SQL, and ETL (Extract, Transform, Load) tools.\n3. **Data Mining**: Focuses on exploratory data analysis, classification, clustering, and regression. Tools used include Pandas, NumPy, Scikit-learn, and Matplotlib.\n4. **Data Visualization**: Covers data visualization techniques, such as bar charts, scatter plots, and heatmaps. Tools include Matplotlib, Seaborn, and Plotly.\n5. **Text Mining**: Focuses on text analysis, sentiment analysis, and topic modeling. Tools used include NLTK (Natural Language Toolkit), TextBlob, and spaCy.\n6. **Machine Learning**: Covers supervised and unsupervised machine learning techniques, including regression, classification, clustering, and neural networks. Tools used include Scikit-learn, TensorFlow, and PyTorch.\n\nThese projects provide a comprehensive understanding of data management concepts and their applications in data science.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Q2.2 Fine-tune the model on course lecture transcripts with LoRA.","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\nfrom peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:19.270313Z","iopub.execute_input":"2025-05-07T20:03:19.271002Z","iopub.status.idle":"2025-05-07T20:03:23.382127Z","shell.execute_reply.started":"2025-05-07T20:03:19.270975Z","shell.execute_reply":"2025-05-07T20:03:23.381501Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"transcript_dir = \"transcripts/transcripts\"\ntest_ratio = 0.1\ntrain_texts = []\ntest_texts = []\ntexts = []\n\nfor file_name in os.listdir(transcript_dir):\n    if file_name.endswith(\".txt\"):\n        with open(os.path.join(transcript_dir, file_name), \"r\", encoding=\"utf-8\") as f:\n          lines = f.readlines()\n          \n          split_idx = int(len(lines) * test_ratio)\n          test_lines = lines[:split_idx]\n          train_lines = lines[split_idx:]\n          train_texts.append(\"\".join(train_lines))\n          test_texts.append(\"\".join(test_lines))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:32.713843Z","iopub.execute_input":"2025-05-07T20:03:32.714134Z","iopub.status.idle":"2025-05-07T20:03:32.722693Z","shell.execute_reply.started":"2025-05-07T20:03:32.714114Z","shell.execute_reply":"2025-05-07T20:03:32.722115Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train_dataset = Dataset.from_dict({\"text\": train_texts})\ntest_dataset = Dataset.from_dict({\"text\": test_texts})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:35.676632Z","iopub.execute_input":"2025-05-07T20:03:35.676973Z","iopub.status.idle":"2025-05-07T20:03:35.694989Z","shell.execute_reply.started":"2025-05-07T20:03:35.676949Z","shell.execute_reply":"2025-05-07T20:03:35.694377Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def tokenize_data(data):\n    tokenized = tokenizer(\n        data[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n    )\n    # Set the labels to be the same as input_ids for causal language modeling\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_train = train_dataset.map(tokenize_data, batched=True)\ntokenized_test = test_dataset.map(tokenize_data, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:38.143358Z","iopub.execute_input":"2025-05-07T20:03:38.143921Z","iopub.status.idle":"2025-05-07T20:03:38.745261Z","shell.execute_reply.started":"2025-05-07T20:03:38.143896Z","shell.execute_reply":"2025-05-07T20:03:38.744511Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b939c46bbd6645abbaaa3ae4f175c8fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"780857629023440bbac8cde35cf8279a"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"tokenized_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:42.450396Z","iopub.execute_input":"2025-05-07T20:03:42.450685Z","iopub.status.idle":"2025-05-07T20:03:42.455843Z","shell.execute_reply.started":"2025-05-07T20:03:42.450663Z","shell.execute_reply":"2025-05-07T20:03:42.455089Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 23\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"tokenized_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:44.864926Z","iopub.execute_input":"2025-05-07T20:03:44.865676Z","iopub.status.idle":"2025-05-07T20:03:44.869959Z","shell.execute_reply.started":"2025-05-07T20:03:44.865649Z","shell.execute_reply":"2025-05-07T20:03:44.869401Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 23\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=8,\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:47.045995Z","iopub.execute_input":"2025-05-07T20:03:47.046287Z","iopub.status.idle":"2025-05-07T20:03:47.050233Z","shell.execute_reply.started":"2025-05-07T20:03:47.046264Z","shell.execute_reply":"2025-05-07T20:03:47.049603Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"training_args = TrainingArguments(\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=1,\n    logging_dir=\"./logs\",\n    output_dir=\"./results\",\n    save_total_limit=2,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:51.462936Z","iopub.execute_input":"2025-05-07T20:03:51.463226Z","iopub.status.idle":"2025-05-07T20:03:51.491207Z","shell.execute_reply.started":"2025-05-07T20:03:51.463204Z","shell.execute_reply":"2025-05-07T20:03:51.490630Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    args=training_args,\n    peft_config=lora_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:03:56.067682Z","iopub.execute_input":"2025-05-07T20:03:56.068013Z","iopub.status.idle":"2025-05-07T20:03:56.863391Z","shell.execute_reply.started":"2025-05-07T20:03:56.067993Z","shell.execute_reply":"2025-05-07T20:03:56.862773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/23 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789767be7fcc497bae288a4559e66124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/23 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1d462df2624c83a22dc73e87b69994"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:04:00.407548Z","iopub.execute_input":"2025-05-07T20:04:00.408104Z","iopub.status.idle":"2025-05-07T20:07:05.610721Z","shell.execute_reply.started":"2025-05-07T20:04:00.408070Z","shell.execute_reply":"2025-05-07T20:07:05.610132Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 03:02, Epoch 8/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.930200</td>\n      <td>2.968326</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.847600</td>\n      <td>2.902081</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.636100</td>\n      <td>2.850378</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.621300</td>\n      <td>2.831767</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.633900</td>\n      <td>2.826171</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.414400</td>\n      <td>2.829195</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.342600</td>\n      <td>2.834560</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.266600</td>\n      <td>2.843247</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=50, training_loss=2.6122322988510134, metrics={'train_runtime': 184.5846, 'train_samples_per_second': 1.246, 'train_steps_per_second': 0.271, 'total_flos': 577309237051392.0, 'train_loss': 2.6122322988510134})"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"Q2.3: Test the model after fine-tuning.","metadata":{}},{"cell_type":"code","source":"prompt = \"Can you tell me what the six projects covered in CS 639 Data Management for Data Science are and what tools each one focuses on in less than a 100 words?\"\nrole = \"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\napply_chat_template(role, prompt, max_new_tokens=300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:07:35.050845Z","iopub.execute_input":"2025-05-07T20:07:35.051367Z","iopub.status.idle":"2025-05-07T20:07:47.430249Z","shell.execute_reply.started":"2025-05-07T20:07:35.051343Z","shell.execute_reply":"2025-05-07T20:07:47.429520Z"}},"outputs":[{"name":"stdout","text":"Role: You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\nPrompt: Can you tell me what the six projects covered in CS 639 Data Management for Data Science are and what tools each one focuses on in less than a 100 words?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nCan you tell me what the six projects covered in CS 639 Data Management for Data Science are and what tools each one focuses on in less than a 100 words?assistant\n\nIn CS 639, the six projects cover the following topics and focus on specific tools. Here's a brief overview of each project:\n\n1. **Data Cleaning**: Focuses on data preprocessing, handling missing values, and data normalization. Utilizes tools like Pandas for data manipulation, NumPy for numerical computations, and Matplotlib for data visualization.\n2. **Data Mixture Modeling**: Covers topic modeling, which involves representing text data as a mixture of different topics. Focuses on tools like Gensim for topic modeling, NLTK for text preprocessing, and Scikit-learn for topic modeling.\n3. **Data Mining**: Covers supervised and unsupervised learning, including clustering, classification, and regression. Focuses on tools like Scikit-learn for machine learning, and Matplotlib and Scipy for data visualization.\n4. **Data Visualization**: Focuses on creating visualizations to represent data insights. Utilizes tools like Matplotlib and Seaborn for data visualization.\n5. **Data Warehousing**: Covers the process of designing and implementing data warehouses. Focuses on tools like Snowflake for data warehousing, and SQL for data querying.\n6. **Data Mining and Data Science**: Covers unsupervised and supervised learning, including clustering, classification, and regression. Focuses on tools like Scikit-learn, and Matplotlib and Scipy for data visualization.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"There seems to be a very slight improvement in the quality of the response.\nThe initial response felt very generic and like something that was just picked up from the internet. \nAfter fine-tuning, the model does seem to pick up some context from the lecture transcripts since it managed to identify that data warehousing was one of the projects and that Snowflake was used in this project,something which wasn't picked up at all initially.\nHowever, there is still a lot of scope for improvement and the response isn't particularly accurate still.","metadata":{}},{"cell_type":"markdown","source":"Q3.3: Compare Fine-Tuning vs RAG","metadata":{}},{"cell_type":"code","source":"queries = [\n    \"What are some of the topics covered in the course?\",\n    \"What is the course code number for Data Management for Data Science?\",\n    \"What NoSQL database was the main focus of project P2 in CS639?\",\n    \"Can you explain window functions as taught in this course?\",\n    \"What SQL query clauses did we learn about in this course?\"\n]\n\nrole = \"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\n\nfor query in queries:\n    apply_chat_template(role, query, max_new_tokens=300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:11:10.942628Z","iopub.execute_input":"2025-05-07T20:11:10.943003Z","iopub.status.idle":"2025-05-07T20:11:52.331010Z","shell.execute_reply.started":"2025-05-07T20:11:10.942979Z","shell.execute_reply":"2025-05-07T20:11:52.330211Z"}},"outputs":[{"name":"stdout","text":"Role: You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\nPrompt: What are some of the topics covered in the course?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nWhat are some of the topics covered in the course?assistant\n\nThe CS 639 Data Management for Data Science course covers a wide range of topics related to data management. Here are some of the key topics that are typically covered in the course:\n\n1. **Data Modeling**: Students learn about data modeling, which is the process of designing and structuring the database schema to support the data requirements of the application. They learn about entities, attributes, relationships, and data types.\n\n2. **Database Design**: Students learn about the design of the database, including the choice of data types, indexing, and the organization of the database structure.\n\n3. **SQL**: SQL is covered as the primary query language used for managing relational databases. Students learn about the basic SQL commands, including SELECT, INSERT, UPDATE, and DELETE.\n\n4. **Database Management Systems**: Students learn about the different types of database management systems (DBMS), including relational, NoSQL, and object-oriented databases. They also learn about the differences between the various DBMS, such as MySQL, PostgreSQL, and MongoDB.\n\n5. **Data Warehouse**: Students learn about data warehouse concepts, including data integration, data transformation, and data storage. They also learn about the different types of data warehouses, such as star and snowflake.\n\n6. **Data Mining**: Students learn about data mining, which is the process of discovering patterns in large datasets. They learn about the different types of data mining techniques, including clustering, classification, and regression.\n\n7. **Data Visualization**: Students learn about data visualization,\nRole: You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\nPrompt: What is the course code number for Data Management for Data Science?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nWhat is the course code number for Data Management for Data Science?assistant\n\nThe course code number for Data Management for Data Science is CS 639.\nRole: You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\nPrompt: What NoSQL database was the main focus of project P2 in CS639?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nWhat NoSQL database was the main focus of project P2 in CS639?assistant\n\nProject P2 was primarily focused on MongoDB.\nRole: You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\nPrompt: Can you explain window functions as taught in this course?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nCan you explain window functions as taught in this course?assistant\n\nWindow functions, also known as aggregate functions, are a fundamental concept in data analysis that allow you to perform calculations on a subset of your data, while keeping the rest of the data intact. They are typically used to answer specific questions about your data.\n\nWindow functions are typically defined over a partitioned dataset, which means that you need to specify the partition boundary. This can be a date, a timestamp, or any other sort of identifier. For example, let's say you have a dataset of exam scores, and you want to know how many students have scored above the 50th percentile on a particular exam. You would specify the partition boundary as the exam score, and then you can use the `PERCENTILE_CONT` function to calculate the 50th percentile.\n\nThe key characteristics of window functions are:\n\n1. **Partitioning**: You need to specify the partition boundary, which is the identifier that defines the subset of your data that you want to process.\n2. **Aggregate**: You need to specify what type of calculation you want to perform. This can be anything from sum, average, count, to name a few.\n3. **Order**: The order of the rows within the partition is important. If you don't specify an order, the default order will be the partition boundary. So, you need to specify the order as well, for example, by using the `ORDER BY` clause.\n\nLet's take a look at some examples. I'm going to use a sample\nRole: You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\nPrompt: What SQL query clauses did we learn about in this course?\nGenerated Response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 07 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nWhat SQL query clauses did we learn about in this course?assistant\n\nWe learned about the following SQL query clauses in this course:\n\n1. **SELECT**: This is the most commonly used clause in SQL. It's used to specify what data you want to retrieve from your database.\n2. **FROM**: This clause is used to specify which table(s) you want to retrieve data from. You need to specify the name of the table, not the name of the database.\n3. **WHERE**: This clause is used to filter the data you want to retrieve. It's used to specify the conditions that you want to apply to your data.\n4. **AND**: The AND operator is used to combine multiple conditions and return the data that meets all of them.\n5. **OR**: The OR operator is used to combine multiple conditions and return the data that meets any of them.\n6. **NOT**: The NOT operator is used to negate a condition. It's often used to exclude data from being returned.\n7. **IN**: The IN operator is used to filter data based on whether it meets a particular condition. It's often used to check if a value is in a list.\n8. **LIKE**: The LIKE operator is used to search for data based on the pattern of the column value. It's often used to search for data based on a specific format.\n9. **BETWEEN**: The BETWEEN operator is used to specify the range of values that you want to filter data based on. It's often used to specify the range of values that\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}